{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './train_data'\n",
    "df_train = pd.read_csv('./df_train.csv')\n",
    "\n",
    "val_path = './validation_data'\n",
    "df_val = pd.read_csv('./df_validation.csv')\n",
    "\n",
    "small_path = './small_sample'\n",
    "df_small = pd.read_csv('./df_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batch_images(images, df, path):\n",
    "    features_target = []\n",
    "    for im in images:\n",
    "        current_image = np.array(imageio.imread(os.path.join(path, im)))/255\n",
    "        image_id = int(im.split('.')[0])\n",
    "        target = int(df[df.id == image_id]['broken'])\n",
    "        features_target.append((current_image, target))\n",
    "    return features_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(df, path, batch_size):\n",
    "    images_name = [f for f in os.listdir(path) if f.split('.')[1] == 'png']\n",
    "    random.shuffle(images_name)\n",
    "    n = len(df)\n",
    "    for i in range(0, n, batch_size):\n",
    "        images_filter = images_name[i:(i+batch_size)]\n",
    "        data = read_batch_images(images_filter, df, path)\n",
    "        yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 3)\n",
    "        self.conv3 = nn.Conv2d(16, 8, 3)\n",
    "        self.fc1 = nn.Linear(12800,512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drp1 = nn.Dropout(0.25)\n",
    "        self.drp2 = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        fo = self.pool(self.relu(self.conv1(x)))\n",
    "        so = self.pool(self.relu(self.conv2(fo)))\n",
    "        to = self.pool(self.relu(self.conv3(so))).view(-1,12800)\n",
    "        fc1_out = self.drp1(self.relu(self.fc1(to)))\n",
    "        fc2_out = self.drp2(self.relu(self.fc2(fc1_out)))\n",
    "        out = self.fc3(fc2_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, df_train, train_path, df_val, val_path, num_epochs, batch_size, device):\n",
    "    for i in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        print(\"epoch \" + str(i))\n",
    "        for batch in batch_generator(df_train, train_path, batch_size):\n",
    "            x = [d[0] for d in batch]\n",
    "            print(len(x))\n",
    "            y = [d[1] for d in batch]\n",
    "            n_batch_size = len(x) if len(x) < batch_size else batch_size\n",
    "            x_t = torch.tensor(x, dtype=torch.float, device=device).view(n_batch_size, 1, 340, 340)\n",
    "            y_t = torch.tensor(y, dtype=torch.float, device=device)\n",
    "            model.zero_grad()\n",
    "            output = model(x_t)\n",
    "            loss = criterion(output.view(-1), y_t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        val_probs, ys, val_loss = validate(model, criterion, val_data, batch_size, device)\n",
    "        val_losses += [val_loss / len(val_data)]\n",
    "        tr_losses += [total_loss / len(train_data)]\n",
    "        print('Epoch {}, avg train loss per image {}, avg valid loss per image {}'.format(\n",
    "            i+1, tr_losses[-1], val_losses[-1]\n",
    "        ))\n",
    "    return total_loss, val_loss\n",
    "\n",
    "def validate(model, criterion, df_val, val_data, batch_size, device):\n",
    "    model.eval()\n",
    "    val_total_loss = 0\n",
    "    probs = []\n",
    "    ys = []\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    with torch.no_grad():\n",
    "        for batch in batch_generator(df_val, val_data, batch_size):\n",
    "            x = [d[0] for d in batch]\n",
    "            y = [d[1] for d in batch]\n",
    "            n_batch_size = len(x) if len(x) < batch_size else batch_size\n",
    "            x_t = torch.tensor(x, dtype=torch.float, device=device).view(n_batch_size, 1, 340, 340)\n",
    "            y_t = torch.tensor(y, dtype=torch.float, device=device)\n",
    "            output = model(x_t)\n",
    "            probs += list(sigmoid(output).view(-1).detach().cpu().numpy())\n",
    "            ys += y\n",
    "            loss = criterion(output.view(-1), y_t)\n",
    "            val_total_loss += loss.item()\n",
    "        return probs, ys, val_total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "device = torch.device('cuda')\n",
    "model = BasicNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-264c03184230>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_small\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./small_sample'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_small\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./small_sample'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-eea7df3707b3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, df_train, train_path, df_val, val_path, num_epochs, batch_size, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mval_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mval_losses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtr_losses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_data' is not defined"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, df_small, './small_sample', df_small, './small_sample', num_epochs=epochs, batch_size=4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-af7a5b056b6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dim' is not defined"
     ]
    }
   ],
   "source": [
    "d = [[1,2], [5,4]]\n",
    "dim(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
