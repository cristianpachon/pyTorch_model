{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from scipy.ndimage import rotate\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './train_data'\n",
    "df_train = pd.read_csv('./df_train.csv')\n",
    "\n",
    "val_path = './validation_data'\n",
    "df_val = pd.read_csv('./df_validation.csv')\n",
    "\n",
    "small_path = './small_sample'\n",
    "df_small = pd.read_csv('./df_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batch_images(images, df, path):\n",
    "    features_target = []\n",
    "    for im in images:\n",
    "        current_image = np.array(imageio.imread(os.path.join(path, im)))/255\n",
    "        image_id = int(im.split('.')[0])\n",
    "        target = int(df[df.id == image_id]['broken'])\n",
    "        features_target.append((current_image, target))\n",
    "    return features_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(df, path, batch_size):\n",
    "    images_name = [f for f in os.listdir(path) if f.split('.')[1] == 'png']\n",
    "    random.shuffle(images_name)\n",
    "    n = len(df)\n",
    "    for i in range(0, n, batch_size):\n",
    "        images_filter = images_name[i:(i+batch_size)]\n",
    "        data = read_batch_images(images_filter, df, path)\n",
    "        yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 3)\n",
    "        self.conv3 = nn.Conv2d(16, 8, 3)\n",
    "        self.fc1 = nn.Linear(12800,5000)\n",
    "        self.fc2 = nn.Linear(5000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 128)\n",
    "        self.fc4 = nn.Linear(128, 1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drp1 = nn.Dropout(0.25)\n",
    "        self.drp2 = nn.Dropout(0.25)\n",
    "        self.drp3 = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        fo = self.pool(self.relu(self.conv1(x)))\n",
    "        so = self.pool(self.relu(self.conv2(fo)))\n",
    "        to = self.pool(self.relu(self.conv3(so))).view(-1,12800)\n",
    "        fc1_out = self.drp1(self.relu(self.fc1(to)))\n",
    "        fc2_out = self.drp2(self.relu(self.fc2(fc1_out)))\n",
    "        fc3_out = self.drp3(self.relu(self.fc3(fc2_out)))\n",
    "        out = self.fc4(fc3_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_metrics(target, score):\n",
    "    fpr, tpr, threshold = roc_curve(target, score)\n",
    "    tnr = [1-f for f in fpr]\n",
    "    auc_roc = roc_auc_score(target, score)\n",
    "    df = pd.DataFrame({'sens' : tpr, 'spec': tnr, 'threshold': threshold})\n",
    "    good_metrics = df[df.sens >= 0.85] \n",
    "    index_best = np.argmax(good_metrics.spec)\n",
    "    df_best = good_metrics.iloc[index_best]\n",
    "    best_sens = df_best.sens\n",
    "    best_spec = df_best.spec\n",
    "    best_th = df_best.threshold\n",
    "\n",
    "    return  auc_roc, best_sens, best_spec, best_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation(img, angle, bg_patch=(5,5)):\n",
    "    bg_color = np.mean(img[:bg_patch[0], :bg_patch[1]])\n",
    "    img = rotate(img, angle, reshape=False)\n",
    "    mask = [img <= 0, np.any(img <= 0, axis=-1)][False]\n",
    "    img[mask] = bg_color\n",
    "    return img\n",
    "\n",
    "def gaussian_noise(img, mean=0, sigma=0.03):\n",
    "    img = img.copy()\n",
    "    noise = np.random.normal(mean, sigma, img.shape)\n",
    "    mask_overflow_upper = img+noise >= 1.0\n",
    "    mask_overflow_lower = img+noise < 0\n",
    "    noise[mask_overflow_upper] = 1.0\n",
    "    noise[mask_overflow_lower] = 0\n",
    "    img += noise\n",
    "    return img\n",
    "\n",
    "def data_augmentation(batch):\n",
    "    data_augmented = []\n",
    "    for x, y in batch:\n",
    "        x_new = 255 * x\n",
    "\n",
    "        p = random.uniform(0, 1)\n",
    "        if p <= 0.5:\n",
    "            angle = random.uniform(0, 360)\n",
    "            x_new = rotation(x_new, angle, bg_patch=(5,5))\n",
    "\n",
    "        p = random.uniform(0, 1)\n",
    "        if p<= 0.5:\n",
    "            mean = random.uniform(0, 1)\n",
    "            sigma = random.uniform(0.001, 0.05)\n",
    "            x_new = gaussian_noise(x_new, mean, sigma)\n",
    "        x_new = x_new/255\n",
    "        data_augmented.append((x_new, y))\n",
    "    return data_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, df_train, train_path, df_val, val_path, num_epochs, batch_size, device):\n",
    "    for i in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_train_images = 0\n",
    "        val_losses = []\n",
    "        tr_losses = []\n",
    "        model.train()\n",
    "        for batch in batch_generator(df_train, train_path, batch_size):\n",
    "            x = [d[0] for d in batch]\n",
    "            y = [d[1] for d in batch]\n",
    "            data_augmented = data_augmentation(batch)\n",
    "            x_augmented = [d[0] for d in data_augmented]\n",
    "            y_augmented = [d[1] for d in data_augmented]\n",
    "            x.extend(x_augmented)\n",
    "            y.extend(y_augmented)\n",
    "            n_batch_size = len(x)\n",
    "            total_train_images += n_batch_size\n",
    "            x_t = torch.tensor(x, dtype=torch.float, device=device).view(n_batch_size, 1, 340, 340)\n",
    "            y_t = torch.tensor(y, dtype=torch.float, device=device)\n",
    "            model.zero_grad()\n",
    "            output = model(x_t)\n",
    "            loss = criterion(output.view(-1), y_t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        val_probs, ys, val_loss = validate(model, criterion, df_val, val_path, batch_size, device)\n",
    "        val_losses += [val_loss / df_val.shape[0]]\n",
    "        tr_losses += [total_loss / total_train_images]\n",
    "        auc_roc, sens, spec, th = get_performance_metrics(ys, val_probs)\n",
    "        \n",
    "        print('Epoch {}, avg train loss per image {}, avg valid loss per image {}, auc {}, sens {}, spec {}, th {}. Train images {}'.format(\n",
    "            i+1, tr_losses[-1], val_losses[-1], auc_roc, sens, spec, th, total_train_images\n",
    "        ))\n",
    "    return tr_losses[-1], val_losses[-1]\n",
    "\n",
    "def validate(model, criterion, df_val, val_path, batch_size, device):\n",
    "    model.eval()\n",
    "    val_total_loss = 0\n",
    "    probs = []\n",
    "    ys = []\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    with torch.no_grad():\n",
    "        for batch in batch_generator(df_val, val_path, batch_size):\n",
    "            x = [d[0] for d in batch]\n",
    "            y = [d[1] for d in batch]\n",
    "            n_batch_size = len(x) if len(x) < batch_size else batch_size\n",
    "            x_t = torch.tensor(x, dtype=torch.float, device=device).view(n_batch_size, 1, 340, 340)\n",
    "            y_t = torch.tensor(y, dtype=torch.float, device=device)\n",
    "            output = model(x_t)\n",
    "            probs += list(sigmoid(output).view(-1).detach().cpu().numpy())\n",
    "            ys += y\n",
    "            loss = criterion(output.view(-1), y_t)\n",
    "            val_total_loss += loss.item()\n",
    "    return probs, ys, val_total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "device = torch.device('cuda')\n",
    "model = BasicNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, avg train loss per image 1.6378302311897277, avg valid loss per image 0.6509859466552734, auc 0.819047619047619, sens 0.8666666666666667, spec 0.5428571428571429, th 0.4425000846385956. Train images 100\n",
      "Epoch 2, avg train loss per image 0.6093555021286011, avg valid loss per image 0.6157529306411743, auc 0.9047619047619048, sens 0.8666666666666667, spec 0.8, th 0.16962794959545135. Train images 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6093555021286011, 0.6157529306411743)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model=model, optimizer=optimizer, criterion=criterion, \n",
    "      df_train=df_train, train_path='./train_data', df_val=df_val, val_path='./validation_data', \n",
    "      num_epochs=epochs, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
