{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import random\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from scipy.ndimage import rotate\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manipulation\n",
    "In this part we split the data into three different datasets:\n",
    "- training\n",
    "- validation\n",
    "-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), 'images_train')\n",
    "csv_file = os.path.join(os.getcwd(), 'data.csv')\n",
    "df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, validation_test_set = train_test_split(df, test_size = 0.3, random_state = 42)\n",
    "validation_set, test_set = train_test_split(validation_test_set, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving the images to different folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_images(df, output_path):\n",
    "    images = df.id\n",
    "    for im in images:\n",
    "        shutil.copy2(os.path.join('./images_train', '{}.png'.format(im)), output_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_images(train_set,'./train_data')\n",
    "move_images(validation_set,'./validation_data')\n",
    "move_images(test_set,'./test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set) + len(validation_set) + len(test_set) == len(df))\n",
    "print(np.mean(df.broken))\n",
    "print(np.mean(train_set.broken))\n",
    "print(np.mean(test_set.broken))\n",
    "print(np.mean(validation_set.broken))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './train_data'\n",
    "df_train = pd.read_csv('./df_train.csv')\n",
    "\n",
    "val_path = './validation_data'\n",
    "df_val = pd.read_csv('./df_validation.csv')\n",
    "\n",
    "small_path = './small_sample'\n",
    "df_small = pd.read_csv('./df_small.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch reading process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batch_images(images, df, path):\n",
    "    features_target = []\n",
    "    for im in images:\n",
    "        current_image = np.array(imageio.imread(os.path.join(path, im)))/255\n",
    "        image_id = int(im.split('.')[0])\n",
    "        target = int(df[df.id == image_id]['broken'])\n",
    "        features_target.append((current_image, target))\n",
    "    return features_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(df, path, batch_size):\n",
    "    images_name = [f for f in os.listdir(path) if f.split('.')[1] == 'png']\n",
    "    random.shuffle(images_name)\n",
    "    n = len(df)\n",
    "    for i in range(0, n, batch_size):\n",
    "        images_filter = images_name[i:(i+batch_size)]\n",
    "        data = read_batch_images(images_filter, df, path)\n",
    "        yield data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation(img, angle, bg_patch):\n",
    "    bg_color = np.mean(img[:bg_patch[0], :bg_patch[1]])\n",
    "    img = rotate(img, angle, reshape=False)\n",
    "    mask = [img <= 0, np.any(img <= 0, axis=-1)][False]\n",
    "    img[mask] = bg_color\n",
    "    return img\n",
    "\n",
    "def gaussian_noise(img, mean, sigma):\n",
    "    img = img.copy()\n",
    "    noise = np.random.normal(mean, sigma, img.shape)\n",
    "    mask_overflow_upper = img+noise >= 1.0\n",
    "    mask_overflow_lower = img+noise < 0\n",
    "    noise[mask_overflow_upper] = 1.0\n",
    "    noise[mask_overflow_lower] = 0\n",
    "    img += noise\n",
    "    return img\n",
    "\n",
    "def translate(img, direction, shift, roll):\n",
    "    img = img.copy()\n",
    "\n",
    "    if direction == 'right':\n",
    "        right_slice = img[:, -shift:].copy()\n",
    "        img[:, shift:] = img[:, :-shift]\n",
    "        if roll:\n",
    "            img[:,:shift] = np.fliplr(right_slice)\n",
    "    if direction == 'left':\n",
    "        left_slice = img[:, :shift].copy()\n",
    "        img[:, :-shift] = img[:, shift:]\n",
    "        if roll:\n",
    "            img[:, -shift:] = left_slice\n",
    "    if direction == 'down':\n",
    "        down_slice = img[-shift:, :].copy()\n",
    "        img[shift:, :] = img[:-shift,:]\n",
    "        if roll:\n",
    "            img[:shift, :] = down_slice\n",
    "    if direction == 'up':\n",
    "        upper_slice = img[:shift, :].copy()\n",
    "        img[:-shift, :] = img[shift:, :]\n",
    "        if roll:\n",
    "            img[-shift:,:] = upper_slice\n",
    "    return img\n",
    "\n",
    "def gaussian_noise(img, mean=0, sigma=0.03):\n",
    "    img = img.copy()\n",
    "    noise = np.random.normal(mean, sigma, img.shape)\n",
    "    mask_overflow_upper = img+noise >= 1.0\n",
    "    mask_overflow_lower = img+noise < 0\n",
    "    noise[mask_overflow_upper] = 1.0\n",
    "    noise[mask_overflow_lower] = 0\n",
    "    img += noise\n",
    "    return img\n",
    "\n",
    "def data_augmentation(batch):\n",
    "    data_augmented = []\n",
    "    for x, y in batch:\n",
    "        x_new = 255 * x\n",
    "\n",
    "        p = random.uniform(0, 1)\n",
    "        if p <= 0.5:\n",
    "            angle = random.uniform(0, 360)\n",
    "            x_new = rotation(x_new, angle, bg_patch=(5,5))\n",
    "\n",
    "        p = random.uniform(0, 1)\n",
    "        if p<= 0.5:\n",
    "            mean = random.uniform(0, 1)\n",
    "            sigma = random.uniform(0.001, 0.05)\n",
    "            x_new = gaussian_noise(x_new, mean, sigma)\n",
    "            \n",
    "        p = random.uniform(0, 1)\n",
    "        if p<= 0.5:\n",
    "            all_directions = ['right', 'left', 'down', 'up']\n",
    "            direction = random.choice(all_directions)\n",
    "            shift = random.randint(10, 200)\n",
    "            roll = True if random.uniform(0, 1) <= 0.5 else False\n",
    "            x_new = translate(x_new, direction, shift, roll)\n",
    "        \n",
    "        x_new = x_new/255\n",
    "        data_augmented.append((x_new, y))\n",
    "    \n",
    "    return data_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_metrics(target, score):\n",
    "    fpr, tpr, threshold = roc_curve(target, score)\n",
    "    tnr = [1-f for f in fpr]\n",
    "    auc_roc = roc_auc_score(target, score)\n",
    "    df = pd.DataFrame({'sens' : tpr, 'spec': tnr, 'threshold': threshold})\n",
    "    good_metrics = df[df.sens >= 0.85] \n",
    "    index_best = np.argmax(good_metrics.spec)\n",
    "    df_best = good_metrics.iloc[index_best]\n",
    "    best_sens = df_best.sens\n",
    "    best_spec = df_best.spec\n",
    "    best_th = df_best.threshold\n",
    "\n",
    "    return  auc_roc, best_sens, best_spec, best_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, df_train, train_path, df_val, val_path, num_epochs, batch_size, device):\n",
    "    for i in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_train_images = 0\n",
    "        val_losses = []\n",
    "        tr_losses = []\n",
    "        model.train()\n",
    "        for batch in batch_generator(df_train, train_path, batch_size):\n",
    "            x = [d[0] for d in batch]\n",
    "            y = [d[1] for d in batch]\n",
    "            data_augmented = data_augmentation(batch)\n",
    "            x_augmented = [d[0] for d in data_augmented]\n",
    "            y_augmented = [d[1] for d in data_augmented]\n",
    "            x.extend(x_augmented)\n",
    "            y.extend(y_augmented)\n",
    "            n_batch_size = len(x)\n",
    "            total_train_images += n_batch_size\n",
    "            x_t = torch.tensor(x, dtype=torch.float, device=device).view(n_batch_size, 1, 340, 340)\n",
    "            y_t = torch.tensor(y, dtype=torch.float, device=device)\n",
    "            model.zero_grad()\n",
    "            output = model(x_t)\n",
    "            loss = criterion(output.view(-1), y_t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        val_probs, ys, val_loss = validate(model, criterion, df_val, val_path, batch_size, device)\n",
    "        val_losses += [val_loss / df_val.shape[0]]\n",
    "        tr_losses += [total_loss / total_train_images]\n",
    "        auc_roc, sens, spec, th = get_performance_metrics(ys, val_probs)\n",
    "        \n",
    "        print('Epoch {}, avg train loss per image {}, avg valid loss per image {}, auc {}, sens {}, spec {}, th {}. Train images {}'.format(\n",
    "            i+1, tr_losses[-1], val_losses[-1], auc_roc, sens, spec, th, total_train_images\n",
    "        ))\n",
    "    return tr_losses[-1], val_losses[-1]\n",
    "\n",
    "def validate(model, criterion, df_val, val_path, batch_size, device):\n",
    "    model.eval()\n",
    "    val_total_loss = 0\n",
    "    probs = []\n",
    "    ys = []\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    with torch.no_grad():\n",
    "        for batch in batch_generator(df_val, val_path, batch_size):\n",
    "            x = [d[0] for d in batch]\n",
    "            y = [d[1] for d in batch]\n",
    "            n_batch_size = len(x) if len(x) < batch_size else batch_size\n",
    "            x_t = torch.tensor(x, dtype=torch.float, device=device).view(n_batch_size, 1, 340, 340)\n",
    "            y_t = torch.tensor(y, dtype=torch.float, device=device)\n",
    "            output = model(x_t)\n",
    "            probs += list(sigmoid(output).view(-1).detach().cpu().numpy())\n",
    "            ys += y\n",
    "            loss = criterion(output.view(-1), y_t)\n",
    "            val_total_loss += loss.item()\n",
    "    return probs, ys, val_total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Net definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 3)\n",
    "        self.conv3 = nn.Conv2d(16, 8, 3)\n",
    "        self.fc1 = nn.Linear(12800,5000)\n",
    "        self.fc2 = nn.Linear(5000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 128)\n",
    "        self.fc4 = nn.Linear(128, 1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drp1 = nn.Dropout(0.25)\n",
    "        self.drp2 = nn.Dropout(0.25)\n",
    "        self.drp3 = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        fo = self.pool(self.relu(self.conv1(x)))\n",
    "        so = self.pool(self.relu(self.conv2(fo)))\n",
    "        to = self.pool(self.relu(self.conv3(so))).view(-1,12800)\n",
    "        fc1_out = self.drp1(self.relu(self.fc1(to)))\n",
    "        fc2_out = self.drp2(self.relu(self.fc2(fc1_out)))\n",
    "        fc3_out = self.drp3(self.relu(self.fc3(fc2_out)))\n",
    "        out = self.fc4(fc3_out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 100\n",
    "learning_rate = 0.0001\n",
    "device = torch.device('cuda')\n",
    "model = BasicNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, avg train loss per image 0.45642406929870794, avg valid loss per image 0.41974780029822384, auc 0.7234328781555169, sens 0.8521739130434782, spec 0.5246971109040075, th 0.14905677735805511. Train images 12158\n",
      "Epoch 2, avg train loss per image 0.41872164535961726, avg valid loss per image 0.4004274958934769, auc 0.7732890311600956, sens 0.8521739130434782, spec 0.5507921714818267, th 0.14301922917366028. Train images 12158\n",
      "Epoch 3, avg train loss per image 0.40545979823146844, avg valid loss per image 0.39123577995658926, auc 0.7803679241460353, sens 0.8521739130434782, spec 0.5712954333643989, th 0.135482057929039. Train images 12158\n",
      "Epoch 4, avg train loss per image 0.39740073129836223, avg valid loss per image 0.3872484559749331, auc 0.7903318610964787, sens 0.8521739130434782, spec 0.5917986952469712, th 0.17481300234794617. Train images 12158\n",
      "Epoch 5, avg train loss per image 0.39623628444706144, avg valid loss per image 0.3836948446649272, auc 0.811677944811378, sens 0.8521739130434782, spec 0.6132339235787512, th 0.2108629196882248. Train images 12158\n",
      "Epoch 6, avg train loss per image 0.3872387869261189, avg valid loss per image 0.3749118447761214, auc 0.8145508326917622, sens 0.8521739130434782, spec 0.6262814538676608, th 0.13492608070373535. Train images 12158\n",
      "Epoch 7, avg train loss per image 0.3784320568995249, avg valid loss per image 0.3621276877791902, auc 0.8371976174075125, sens 0.8521739130434782, spec 0.6691519105312209, th 0.13590984046459198. Train images 12158\n",
      "Epoch 8, avg train loss per image 0.36644261776371445, avg valid loss per image 0.3528765916870085, auc 0.8411483447465458, sens 0.8521739130434782, spec 0.6775396085740913, th 0.15870904922485352. Train images 12158\n",
      "Epoch 9, avg train loss per image 0.3603175505738369, avg valid loss per image 0.33722971561589976, auc 0.852242797520159, sens 0.8521739130434782, spec 0.6859273066169618, th 0.11340804398059845. Train images 12158\n",
      "Epoch 10, avg train loss per image 0.34158762106477364, avg valid loss per image 0.31782698580968044, auc 0.863584424004214, sens 0.8521739130434782, spec 0.7222739981360671, th 0.1855628788471222. Train images 12158\n",
      "Epoch 11, avg train loss per image 0.32964132270994967, avg valid loss per image 0.33402131103316546, auc 0.8679200940070506, sens 0.8521739130434782, spec 0.7073625349487418, th 0.08183857798576355. Train images 12158\n",
      "Epoch 12, avg train loss per image 0.3101494863327837, avg valid loss per image 0.29649507880485343, auc 0.8820819320069695, sens 0.8521739130434782, spec 0.733457595526561, th 0.1277121603488922. Train images 12158\n",
      "Epoch 13, avg train loss per image 0.3019948699870849, avg valid loss per image 0.30110583960415305, auc 0.8842132987560274, sens 0.8521739130434782, spec 0.7362534948741846, th 0.09721340239048004. Train images 12158\n",
      "Epoch 14, avg train loss per image 0.28534508504489475, avg valid loss per image 0.32119855652931373, auc 0.8791158474816646, sens 0.8521739130434782, spec 0.7138863000931966, th 0.06715800613164902. Train images 12158\n",
      "Epoch 15, avg train loss per image 0.27599964621898904, avg valid loss per image 0.28291776970726473, auc 0.8980023501762633, sens 0.8521739130434782, spec 0.7232059645852749, th 0.11618196964263916. Train images 12158\n",
      "Epoch 16, avg train loss per image 0.25876202734702314, avg valid loss per image 0.2666826474145112, auc 0.9063333198265732, sens 0.8521739130434782, spec 0.7800559179869525, th 0.15931518375873566. Train images 12158\n",
      "Epoch 17, avg train loss per image 0.257595385115169, avg valid loss per image 0.27874282878449763, auc 0.9027553790672231, sens 0.8521739130434782, spec 0.7651444547996272, th 0.13065581023693085. Train images 12158\n",
      "Epoch 18, avg train loss per image 0.23486592013068372, avg valid loss per image 0.2774422426042608, auc 0.9086632359495928, sens 0.8521739130434782, spec 0.7837837837837838, th 0.07880659401416779. Train images 12158\n",
      "Epoch 19, avg train loss per image 0.23203599317663298, avg valid loss per image 0.27080323342991536, auc 0.9147048097572834, sens 0.8608695652173913, spec 0.8061509785647717, th 0.09073875099420547. Train images 12158\n",
      "Epoch 20, avg train loss per image 0.2231422652733877, avg valid loss per image 0.27302862220055674, auc 0.9150532841687264, sens 0.8521739130434782, spec 0.7958993476234856, th 0.1311594545841217. Train images 12158\n",
      "Epoch 21, avg train loss per image 0.21063623727985778, avg valid loss per image 0.2734128358907546, auc 0.9147736942339642, sens 0.8521739130434782, spec 0.7903075489282386, th 0.12066846340894699. Train images 12158\n",
      "Epoch 22, avg train loss per image 0.21218284491469816, avg valid loss per image 0.27898298572614527, auc 0.9238583410997204, sens 0.8521739130434782, spec 0.8238583410997204, th 0.055847201496362686. Train images 12158\n",
      "Epoch 23, avg train loss per image 0.20372407078291951, avg valid loss per image 0.2773735103846391, auc 0.922861542201872, sens 0.8565217391304348, spec 0.8285181733457596, th 0.06829484552145004. Train images 12158\n",
      "Epoch 24, avg train loss per image 0.18491546008359014, avg valid loss per image 0.28735570073219235, auc 0.9223388305847077, sens 0.8521739130434782, spec 0.8452935694315005, th 0.06280010938644409. Train images 12158\n",
      "Epoch 25, avg train loss per image 0.18338920168430323, avg valid loss per image 0.27162837329751605, auc 0.9252846549698124, sens 0.8521739130434782, spec 0.8462255358807083, th 0.094308041036129. Train images 12158\n",
      "Epoch 26, avg train loss per image 0.19095265839832906, avg valid loss per image 0.28234253859208897, auc 0.9252562907735321, sens 0.8521739130434782, spec 0.8247903075489282, th 0.04695059359073639. Train images 12158\n",
      "Epoch 27, avg train loss per image 0.19143900970895739, avg valid loss per image 0.2749135117300272, auc 0.9275943109526317, sens 0.8521739130434782, spec 0.8611369990680335, th 0.07653352618217468. Train images 12158\n",
      "Epoch 28, avg train loss per image 0.17450300747870928, avg valid loss per image 0.2755677431267175, auc 0.9312451882167025, sens 0.8521739130434782, spec 0.8741845293569431, th 0.0922556221485138. Train images 12158\n",
      "Epoch 29, avg train loss per image 0.17537281022571502, avg valid loss per image 0.28583415354804315, auc 0.9287572430001215, sens 0.8521739130434782, spec 0.8415657036346691, th 0.05984780937433243. Train images 12158\n",
      "Epoch 30, avg train loss per image 0.1624425195278865, avg valid loss per image 0.3188387936119459, auc 0.9254062158110135, sens 0.8521739130434782, spec 0.848089468779124, th 0.029347751289606094. Train images 12158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1624425195278865, 0.3188387936119459)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model=model, optimizer=optimizer, criterion=criterion, \n",
    "      df_train=df_train, train_path='./train_data', df_val=df_val, val_path='./validation_data', \n",
    "      num_epochs=epochs, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type BasicNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, './model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = './test_data'\n",
    "df_test = pd.read_csv('./df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs, test_ys, test_loss = validate(model, criterion, df_test, test_path, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc, sens, spec, th = get_performance_metrics(test_ys, test_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9098937284837599\n",
      "0.8549019607843137\n",
      "0.7566793893129771\n",
      "0.01589040458202362\n"
     ]
    }
   ],
   "source": [
    "print(roc)\n",
    "print(sens)\n",
    "print(spec)\n",
    "print(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batch_images_test(images, path):\n",
    "    list_images = []\n",
    "    for im in images:\n",
    "        current_image = np.array(imageio.imread(os.path.join(path, im)))/255\n",
    "        list_images.append(current_image)\n",
    "    return list_images\n",
    "\n",
    "def batch_generator_test(path, batch_size):\n",
    "    images_name = [f for f in os.listdir(path) if f.split('.')[1] == 'png']\n",
    "    random.shuffle(images_name)\n",
    "    n = len(images_name)\n",
    "    for i in range(0, n, batch_size):\n",
    "        images_filter = images_name[i:(i+batch_size)]\n",
    "        data = read_batch_images_test(images_filter, path)\n",
    "        yield data\n",
    "        \n",
    "def inference(model, path):\n",
    "    batch_size = 100\n",
    "    device = torch.device('cuda')\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    with torch.no_grad():\n",
    "        for batch in batch_generator_test(path, batch_size):\n",
    "            x = [d for d in batch]\n",
    "            n_batch_size = len(x) if len(x) < batch_size else batch_size\n",
    "            x_t = torch.tensor(x, dtype=torch.float, device=device).view(n_batch_size, 1, 340, 340)\n",
    "            output = model(x_t)\n",
    "            probs += list(sigmoid(output).view(-1).detach().cpu().numpy())\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_location = './model_cristian_pachon.pkl'\n",
    "model_trained = torch.load(model_location)\n",
    "model_trained.eval()\n",
    "inference_probs = inference(model_trained, './test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0014983752, 0.06869454, 0.0050477795, 0.38338143, 0.00061182]\n"
     ]
    }
   ],
   "source": [
    "print(inference_probs[:5])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
