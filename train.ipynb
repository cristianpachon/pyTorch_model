{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import random\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from scipy.ndimage import rotate\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manipulation\n",
    "In this part we split the data into three different datasets:\n",
    "- training\n",
    "- validation\n",
    "-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), 'images_train')\n",
    "csv_file = os.path.join(os.getcwd(), 'data.csv')\n",
    "df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, validation_test_set = train_test_split(df, test_size = 0.3, random_state = 42)\n",
    "validation_set, test_set = train_test_split(validation_test_set, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving the images to different folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_images(df, output_path):\n",
    "    images = df.id\n",
    "    for im in images:\n",
    "        shutil.copy2(os.path.join('./images_train', '{}.png'.format(im)), output_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_images(train_set,'./train_data')\n",
    "move_images(validation_set,'./validation_data')\n",
    "move_images(test_set,'./test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set) + len(validation_set) + len(test_set) == len(df))\n",
    "print(np.mean(df.broken))\n",
    "print(np.mean(train_set.broken))\n",
    "print(np.mean(test_set.broken))\n",
    "print(np.mean(validation_set.broken))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './train_data'\n",
    "df_train = pd.read_csv('./df_train.csv')\n",
    "\n",
    "val_path = './validation_data'\n",
    "df_val = pd.read_csv('./df_validation.csv')\n",
    "\n",
    "small_path = './small_sample'\n",
    "df_small = pd.read_csv('./df_small.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch reading process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batch_images(images, df, path):\n",
    "    features_target = []\n",
    "    for im in images:\n",
    "        current_image = np.array(imageio.imread(os.path.join(path, im)))/255\n",
    "        image_id = int(im.split('.')[0])\n",
    "        target = int(df[df.id == image_id]['broken'])\n",
    "        features_target.append((current_image, target))\n",
    "    return features_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(df, path, batch_size):\n",
    "    images_name = [f for f in os.listdir(path) if f.split('.')[1] == 'png']\n",
    "    random.shuffle(images_name)\n",
    "    n = len(df)\n",
    "    for i in range(0, n, batch_size):\n",
    "        images_filter = images_name[i:(i+batch_size)]\n",
    "        data = read_batch_images(images_filter, df, path)\n",
    "        yield data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation(img, angle, bg_patch):\n",
    "    bg_color = np.mean(img[:bg_patch[0], :bg_patch[1]])\n",
    "    img = rotate(img, angle, reshape=False)\n",
    "    mask = [img <= 0, np.any(img <= 0, axis=-1)][False]\n",
    "    img[mask] = bg_color\n",
    "    return img\n",
    "\n",
    "def gaussian_noise(img, mean, sigma):\n",
    "    img = img.copy()\n",
    "    noise = np.random.normal(mean, sigma, img.shape)\n",
    "    mask_overflow_upper = img+noise >= 1.0\n",
    "    mask_overflow_lower = img+noise < 0\n",
    "    noise[mask_overflow_upper] = 1.0\n",
    "    noise[mask_overflow_lower] = 0\n",
    "    img += noise\n",
    "    return img\n",
    "\n",
    "def translate(img, direction, shift, roll):\n",
    "    img = img.copy()\n",
    "\n",
    "    if direction == 'right':\n",
    "        right_slice = img[:, -shift:].copy()\n",
    "        img[:, shift:] = img[:, :-shift]\n",
    "        if roll:\n",
    "            img[:,:shift] = np.fliplr(right_slice)\n",
    "    if direction == 'left':\n",
    "        left_slice = img[:, :shift].copy()\n",
    "        img[:, :-shift] = img[:, shift:]\n",
    "        if roll:\n",
    "            img[:, -shift:] = left_slice\n",
    "    if direction == 'down':\n",
    "        down_slice = img[-shift:, :].copy()\n",
    "        img[shift:, :] = img[:-shift,:]\n",
    "        if roll:\n",
    "            img[:shift, :] = down_slice\n",
    "    if direction == 'up':\n",
    "        upper_slice = img[:shift, :].copy()\n",
    "        img[:-shift, :] = img[shift:, :]\n",
    "        if roll:\n",
    "            img[-shift:,:] = upper_slice\n",
    "    return img\n",
    "\n",
    "def gaussian_noise(img, mean=0, sigma=0.03):\n",
    "    img = img.copy()\n",
    "    noise = np.random.normal(mean, sigma, img.shape)\n",
    "    mask_overflow_upper = img+noise >= 1.0\n",
    "    mask_overflow_lower = img+noise < 0\n",
    "    noise[mask_overflow_upper] = 1.0\n",
    "    noise[mask_overflow_lower] = 0\n",
    "    img += noise\n",
    "    return img\n",
    "\n",
    "def data_augmentation(batch):\n",
    "    data_augmented = []\n",
    "    for x, y in batch:\n",
    "        x_new = 255 * x\n",
    "\n",
    "        p = random.uniform(0, 1)\n",
    "        if p <= 0.5:\n",
    "            angle = random.uniform(0, 360)\n",
    "            x_new = rotation(x_new, angle, bg_patch=(5,5))\n",
    "\n",
    "        p = random.uniform(0, 1)\n",
    "        if p<= 0.5:\n",
    "            mean = random.uniform(0, 1)\n",
    "            sigma = random.uniform(0.001, 0.05)\n",
    "            x_new = gaussian_noise(x_new, mean, sigma)\n",
    "            \n",
    "        p = random.uniform(0, 1)\n",
    "        if p<= 0.5:\n",
    "            all_directions = ['right', 'left', 'down', 'up']\n",
    "            direction = random.choice(all_directions)\n",
    "            shift = random.randint(10, 200)\n",
    "            roll = True if random.uniform(0, 1) <= 0.5 else False\n",
    "            x_new = translate(x_new, direction, shift, roll)\n",
    "        \n",
    "        x_new = x_new/255\n",
    "        data_augmented.append((x_new, y))\n",
    "    \n",
    "    return data_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_metrics(target, score):\n",
    "    fpr, tpr, threshold = roc_curve(target, score)\n",
    "    tnr = [1-f for f in fpr]\n",
    "    auc_roc = roc_auc_score(target, score)\n",
    "    df = pd.DataFrame({'sens' : tpr, 'spec': tnr, 'threshold': threshold})\n",
    "    good_metrics = df[df.sens >= 0.85] \n",
    "    index_best = np.argmax(good_metrics.spec)\n",
    "    df_best = good_metrics.iloc[index_best]\n",
    "    best_sens = df_best.sens\n",
    "    best_spec = df_best.spec\n",
    "    best_th = df_best.threshold\n",
    "\n",
    "    return  auc_roc, best_sens, best_spec, best_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, df_train, train_path, df_val, val_path, num_epochs, batch_size, device):\n",
    "    for i in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_train_images = 0\n",
    "        val_losses = []\n",
    "        tr_losses = []\n",
    "        model.train()\n",
    "        for batch in batch_generator(df_train, train_path, batch_size):\n",
    "            x = [d[0] for d in batch]\n",
    "            y = [d[1] for d in batch]\n",
    "            data_augmented = data_augmentation(batch)\n",
    "            x_augmented = [d[0] for d in data_augmented]\n",
    "            y_augmented = [d[1] for d in data_augmented]\n",
    "            x.extend(x_augmented)\n",
    "            y.extend(y_augmented)\n",
    "            n_batch_size = len(x)\n",
    "            total_train_images += n_batch_size\n",
    "            x_t = torch.tensor(x, dtype=torch.float, device=device).view(n_batch_size, 1, 340, 340)\n",
    "            y_t = torch.tensor(y, dtype=torch.float, device=device)\n",
    "            model.zero_grad()\n",
    "            output = model(x_t)\n",
    "            loss = criterion(output.view(-1), y_t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        val_probs, ys, val_loss = validate(model, criterion, df_val, val_path, batch_size, device)\n",
    "        val_losses += [val_loss / df_val.shape[0]]\n",
    "        tr_losses += [total_loss / total_train_images]\n",
    "        auc_roc, sens, spec, th = get_performance_metrics(ys, val_probs)\n",
    "        \n",
    "        print('Epoch {}, avg train loss per image {}, avg valid loss per image {}, auc {}, sens {}, spec {}, th {}. Train images {}'.format(\n",
    "            i+1, tr_losses[-1], val_losses[-1], auc_roc, sens, spec, th, total_train_images\n",
    "        ))\n",
    "    return tr_losses[-1], val_losses[-1]\n",
    "\n",
    "def validate(model, criterion, df_val, val_path, batch_size, device):\n",
    "    model.eval()\n",
    "    val_total_loss = 0\n",
    "    probs = []\n",
    "    ys = []\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    with torch.no_grad():\n",
    "        for batch in batch_generator(df_val, val_path, batch_size):\n",
    "            x = [d[0] for d in batch]\n",
    "            y = [d[1] for d in batch]\n",
    "            n_batch_size = len(x) if len(x) < batch_size else batch_size\n",
    "            x_t = torch.tensor(x, dtype=torch.float, device=device).view(n_batch_size, 1, 340, 340)\n",
    "            y_t = torch.tensor(y, dtype=torch.float, device=device)\n",
    "            output = model(x_t)\n",
    "            probs += list(sigmoid(output).view(-1).detach().cpu().numpy())\n",
    "            ys += y\n",
    "            loss = criterion(output.view(-1), y_t)\n",
    "            val_total_loss += loss.item()\n",
    "    return probs, ys, val_total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Net definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 3)\n",
    "        self.conv3 = nn.Conv2d(16, 8, 3)\n",
    "        self.fc1 = nn.Linear(12800,5000)\n",
    "        self.fc2 = nn.Linear(5000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 128)\n",
    "        self.fc4 = nn.Linear(128, 1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drp1 = nn.Dropout(0.25)\n",
    "        self.drp2 = nn.Dropout(0.25)\n",
    "        self.drp3 = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        fo = self.pool(self.relu(self.conv1(x)))\n",
    "        so = self.pool(self.relu(self.conv2(fo)))\n",
    "        to = self.pool(self.relu(self.conv3(so))).view(-1,12800)\n",
    "        fc1_out = self.drp1(self.relu(self.fc1(to)))\n",
    "        fc2_out = self.drp2(self.relu(self.fc2(fc1_out)))\n",
    "        fc3_out = self.drp3(self.relu(self.fc3(fc2_out)))\n",
    "        out = self.fc4(fc3_out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 100\n",
    "learning_rate = 0.0001\n",
    "device = torch.device('cuda')\n",
    "model = BasicNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model=model, optimizer=optimizer, criterion=criterion, \n",
    "      df_train=df_train, train_path='./train_data', df_val=df_val, val_path='./validation_data', \n",
    "      num_epochs=epochs, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = './test_data'\n",
    "df_test = pd.read_csv('./df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs, test_ys, test_loss = validate(model, criterion, df_test, test_path, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc, sens, spec, th = get_performance_metrics(test_ys, test_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc)\n",
    "print(sens)\n",
    "print(spec)\n",
    "print(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batch_images_test(images, path):\n",
    "    list_images = []\n",
    "    for im in images:\n",
    "        current_image = np.array(imageio.imread(os.path.join(path, im)))/255\n",
    "        list_images.append(current_image)\n",
    "    return list_images\n",
    "\n",
    "def batch_generator_test(path, batch_size):\n",
    "    images_name = [f for f in os.listdir(path) if f.split('.')[1] == 'png']\n",
    "    random.shuffle(images_name)\n",
    "    n = len(images_name)\n",
    "    for i in range(0, n, batch_size):\n",
    "        images_filter = images_name[i:(i+batch_size)]\n",
    "        data = read_batch_images_test(images_filter, path)\n",
    "        yield data\n",
    "        \n",
    "def inference(model, path):\n",
    "    batch_size = 100\n",
    "    device = torch.device('cuda')\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    with torch.no_grad():\n",
    "        for batch in batch_generator_test(path, batch_size):\n",
    "            x = [d for d in batch]\n",
    "            n_batch_size = len(x) if len(x) < batch_size else batch_size\n",
    "            x_t = torch.tensor(x, dtype=torch.float, device=device).view(n_batch_size, 1, 340, 340)\n",
    "            output = model(x_t)\n",
    "            probs += list(sigmoid(output).view(-1).detach().cpu().numpy())\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_location = './model_cristian_pachon.pkl'\n",
    "model_trained = torch.load(model_location)\n",
    "model_trained.eval()\n",
    "inference_probs = inference(model_trained, './test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inference_probs[:5])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
